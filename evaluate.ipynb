{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate detection and tracking\n",
    "This script evaluates the fish detection and tracking.\n",
    "\n",
    "## Install software\n",
    "To install the software on your own computer, follow the steps provided in the [readme](https://github.com/Rick-v-E/automatic_discard_registration/blob/master/README.md). If running on Google Colab, clone the GIT repository and install it's dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "# Check if the repository is already available, if not, clone and install\n",
    "if [ ! -d .git ]\n",
    "then\n",
    "  git clone --recurse-submodules https://github.com/Rick-v-E/automatic_discard_registration.git\n",
    "  pip install -r automatic_discard_registration/requirements.txt\n",
    "  pip install -r automatic_discard_registration/detection/yolov3/requirements.txt\n",
    "  pip install automatic_discard_registration/detection/apex\n",
    "  pip install gdown\n",
    "else\n",
    "  git pull\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you installed the software in the previous step, enter the repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd automatic_discard_registration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup dataset\n",
    "The complete dataset can be downloaded from [4TU.ResearchData](https://doi.org/10.4121/16622566.v1). To use this dataset, extract both `fdf_images.zip` and `results.zip` in the [data](https://github.com/Rick-v-E/automatic_discard_registration/tree/master/data) folder.\n",
    "\n",
    "For use on Google Colab, we have created a smaller subset of the data. This dataset contains only part of the images of the complete dataset, but contains all result from the complete dataset.\n",
    "\n",
    "---\n",
    "**IMPORTANT**\n",
    "\n",
    "Execute only one of the three cells below! Each cell contains a method to import the data, if one method fails, use another method. If the method succeed, go to the next section in this notebook.\n",
    "\n",
    "---\n",
    "\n",
    "**METHOD 1** Download and extract the sample dataset (this will take around 5-10 minutes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown --id 1TcyeeX0UjhWldbjhLkCRJIuktDNeAMJJ\n",
    "!unzip -q fdf_sample_dataset.zip -d data\n",
    "!rm fdf_sample_dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**METHOD 2** Download the [sample dataset](https://drive.google.com/file/d/1TcyeeX0UjhWldbjhLkCRJIuktDNeAMJJ/view?usp=sharing) manually and upload it to Google Colab in the `automatic_discard_registration` opening the files tab and right click on the folder name:\n",
    "\n",
    "![Manual upload image](colab_manual_upload.png)\n",
    "\n",
    "After uploading, extract the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q fdf_sample_dataset.zip -d data\n",
    "!rm fdf_sample_dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**METHOD 3** Download the [sample dataset](https://drive.google.com/file/d/1TcyeeX0UjhWldbjhLkCRJIuktDNeAMJJ/view?usp=sharing) and upload it to your personal Google Drive account. Connect this account to Google Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!unzip -q ../drive/MyDrive/fdf_sample_dataset.zip -d data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the dataset is loaded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "NEEDED_FOLDERS = [\"fdf_images\", \"results\"]\n",
    "\n",
    "# Check if all folders are correct\n",
    "if not all([(DATA_PATH / f).is_dir() for f in NEEDED_FOLDERS]):\n",
    "    print(\"Could not find all data folders! Did you extract both fdf_images.zip and results.zip in the data folder?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the same results as in the paper, use the complete dataset. Upload the dataset to your Google Drive and [mount](https://towardsdatascience.com/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166) this folder to your Google Colab environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup evaluation notebook\n",
    "Start by importing the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from IPython.display import HTML, display\n",
    "from sklearn.metrics import classification_report\n",
    "from tabulate import tabulate\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from common.io import load_detection_file, load_annotation_files\n",
    "from common.nb_utils import show_mc_precision_recall_curve, show_confusion_matrix\n",
    "from evaluation.fdf_detections import FDFDetectionEvaluator\n",
    "from evaluation.fdf_trackers import count_trackers_by_class\n",
    "\n",
    "rcParams[\"font.family\"] = \"sans-serif\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection evaluation\n",
    "Load the detection files (using 0, 50, 100 and 200 synthetic images):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dict = {\n",
    "    \"detections_validation_0\": Path(\"data/results/model_selection/detections_validation_0_synthetic_images.json\"),\n",
    "    \"detections_validation_50\": Path(\"data/results/model_selection/detections_validation_50_synthetic_images.json\"),\n",
    "    \"detections_validation_100\": Path(\"data/results/model_selection/detections_validation_100_synthetic_images.json\"),\n",
    "    \"detections_validation_200\": Path(\"data/results/model_selection/detections_validation_200_synthetic_images.json\"),\n",
    "    \"detections_test_200\": Path(\"data/results/detections_test.json\"),\n",
    "    \"names_file\": Path(\"detection/fish_classes.names\"),\n",
    "}\n",
    "\n",
    "assert all(f.is_file() for f in path_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the ground truth and the detection files. \n",
    "\n",
    "---\n",
    "**NOTE**\n",
    "\n",
    "The example dataset contains a subset of all images. However, all annotation files are added. The detection files are from the complete dataset, therefore, the results below should be the same as reported in the paper.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_annotation_folder = Path(\"data/fdf_images/annotations/validation\")\n",
    "test_annotation_folder = Path(\"data/fdf_images/annotations/test\")\n",
    "\n",
    "assert validation_annotation_folder.is_dir()\n",
    "assert test_annotation_folder.is_dir()\n",
    "\n",
    "# Load ground truth files\n",
    "validation_ground_truth = load_annotation_files(validation_annotation_folder, skip_classes=[\"dragonet\"])  # There are no dragonets in the validation and test datasets\n",
    "test_ground_truth = load_annotation_files(test_annotation_folder, skip_classes=[\"dragonet\"]) # There are no dragonets in the validation and test datasets\n",
    "\n",
    "# Load detection files\n",
    "detections = {}\n",
    "for n_synthetic_images in tqdm([0, 50, 100, 200], desc=\"Loading subsets\"):\n",
    "    detections[f\"validation_{ n_synthetic_images }\"] = load_detection_file(path_dict[f\"detections_validation_{ n_synthetic_images }\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 4 evaluators (one for each amount of synthetic images):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluators = {}\n",
    "for n_synthetic_images in tqdm([0, 50, 100, 200], desc=\"Create evaluators for subsets\"):\n",
    "    evaluators[f\"validation_{ n_synthetic_images }\"] = FDFDetectionEvaluator(validation_ground_truth, detections[f\"validation_{ n_synthetic_images }\"], path_dict, skip_classes=[\"dragonet\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associate all the detections with their ground truth annotation based on the intersection-over-union (IoU) score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for name, ev in tqdm(evaluators.items(), desc=\"Associate subsets\"):\n",
    "    results[name] = ev.associate_results_with_gt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance measures\n",
    "For each evaluator, calculate the performance measures (F1-sore, precision and recall). The support column indicates the number of fishes with that specific class in the ground truth.\n",
    "\n",
    "---\n",
    "**NOTE**\n",
    "\n",
    "The macro and weighted scores in the paper are recalculated from the F1-score, precision and recall by omitting the background row.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, result in results.items():\n",
    "    print(f\"Classification results for { name.split('_')[1] } synthetic images:\")\n",
    "    print(classification_report(result.y_true, result.y_pred, zero_division=0, target_names=result.classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the performance measures, we see that adding 200 synthetic images yielded the highest F1-score. Therefore, we use this model for the rest of the experiments. Now, calculate the test performance of the 200 synthetic images model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_detections = load_detection_file(path_dict[f\"detections_test_200\"])\n",
    "evaluator = FDFDetectionEvaluator(test_ground_truth, test_detections, path_dict, skip_classes=[\"dragonet\"])\n",
    "result = evaluator.associate_results_with_gt()\n",
    "print(classification_report(result.y_true, result.y_pred, zero_division=0, target_names=result.classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "Create a the confusion matrix. The colors are normalized over the rows (as fraction of the ground truth) and the numbers are the real number of fishes for that specific cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "show_confusion_matrix(result, ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, save the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "show_confusion_matrix(result, ax)\n",
    "plt.subplots_adjust(bottom=0.25, left=0.21)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/confusion_matrix.eps\")\n",
    "# plt.savefig(\"data/confusion_matrix.png\", dpi=600)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall curve\n",
    "Calculate and the multi-class precision-recall curves. The score for each detection is the objectness multiplied by the class probability for a specific class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1, 2, figsize=(12,6))\n",
    "plt.subplots_adjust(wspace=0.7)\n",
    "\n",
    "# Split between high and low frequent fish species\n",
    "lf = [\"dab\", \"gurnard\", \"lesser_spotted_dogfish\", \"pouting\", \"turbot\"]\n",
    "hf = [\"common_sole\", \"lemon_sole\", \"plaice\", \"ray\", \"whiting\"]\n",
    "\n",
    "show_mc_precision_recall_curve(result, lf, axarr[0])\n",
    "show_mc_precision_recall_curve(result, hf, axarr[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, save the figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "show_mc_precision_recall_curve(result, lf, ax, fontsize=17)\n",
    "plt.subplots_adjust(right=0.7)\n",
    "plt.savefig(\"data/precision_recall_lf.eps\")\n",
    "# plt.savefig(\"data/precision_recall_lf.png\", dpi=600)\n",
    "plt.tight_layout()\n",
    "plt.close(fig)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "show_mc_precision_recall_curve(result, hf, ax, fontsize=17)\n",
    "plt.subplots_adjust(right=0.7)\n",
    "plt.savefig(\"data/precision_recall_hf.eps\")\n",
    "# plt.savefig(\"data/precision_recall_hf.png\", dpi=600)\n",
    "plt.tight_layout()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking evaluation\n",
    "The tracking is mainly evaluated by counting the number of trackers for each fish specie and compare this number with the ground truth. This is done for each box individually, one box per day. Each box has 4 runs (repetitions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker_path_dict = {\n",
    "    \"20191018_run_1_tracking\": Path(\"data/results/EM_comparison/20191018_run_1_tracking.json\"),    \n",
    "    \"20191018_run_2_tracking\": Path(\"data/results/EM_comparison/20191018_run_2_tracking.json\"),    \n",
    "    \"20191018_run_3_tracking\": Path(\"data/results/EM_comparison/20191018_run_3_tracking.json\"),    \n",
    "    \"20191018_run_4_tracking\": Path(\"data/results/EM_comparison/20191018_run_4_tracking.json\"),  \n",
    "    \"20191025_run_1_tracking\": Path(\"data/results/EM_comparison/20191025_run_1_tracking.json\"),  \n",
    "    \"20191025_run_2_tracking\": Path(\"data/results/EM_comparison/20191025_run_2_tracking.json\"),  \n",
    "    \"20191025_run_3_tracking\": Path(\"data/results/EM_comparison/20191025_run_3_tracking.json\"),  \n",
    "    \"20191025_run_4_tracking\": Path(\"data/results/EM_comparison/20191025_run_4_tracking.json\"),  \n",
    "    \"20191101_run_1_tracking\": Path(\"data/results/EM_comparison/20191101_run_1_tracking.json\"),  \n",
    "    \"20191101_run_2_tracking\": Path(\"data/results/EM_comparison/20191101_run_2_tracking.json\"),  \n",
    "    \"20191101_run_3_tracking\": Path(\"data/results/EM_comparison/20191101_run_3_tracking.json\"),  \n",
    "    \"20191101_run_4_tracking\": Path(\"data/results/EM_comparison/20191101_run_4_tracking.json\"),    \n",
    "}\n",
    "\n",
    "assert all(f.is_file() for f in tracker_path_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load all the tracker files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker_dict = {}\n",
    "for name, file_path in tqdm(tracker_path_dict.items(), desc=\"Loading all runs\"):\n",
    "    tracker_dict[name] = load_detection_file(file_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of fishes for each specie in each file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the table\n",
    "table = []\n",
    "header = [\"Date\", \"Run\"]\n",
    "header += [class_name.replace(\"_\", \" \").capitalize() for class_name in result.classes if class_name != \"background\"]\n",
    "\n",
    "# Count the total number per specie\n",
    "total = defaultdict(int)\n",
    "\n",
    "for name, t_dict in tracker_dict.items():\n",
    "    counts = count_trackers_by_class(t_dict)    \n",
    "\n",
    "    row = [name.split(\"_\")[0], name.split(\"_\")[2]]\n",
    "    for class_name in result.classes:\n",
    "        if class_name == \"background\":\n",
    "            continue\n",
    "\n",
    "        n = counts.get(class_name, 0)\n",
    "\n",
    "        total[class_name] += n\n",
    "        row.append(n)\n",
    "\n",
    "    table.append(row)\n",
    "\n",
    "total_row = [\"Total\", \"\"]\n",
    "total_row += [total[class_name] for class_name in result.classes if class_name != \"background\"]\n",
    "table.append(total_row)\n",
    "\n",
    "display(HTML(tabulate(table, tablefmt=\"html\", headers=header)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9756859299261392d218088cb417af53d94c7bf0c66d0aa80032af89e52b4e36"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
