{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Track the fish over consequtive images\r\n",
    "This notebook shows how to track the fish over consequtive images using the tracking algorithm.\r\n",
    "\r\n",
    "## Install software\r\n",
    "To install the software on your own computer, follow the steps provided in the [readme](https://github.com/Rick-v-E/automatic_discard_registration/blob/master/README.md). If running on Google Colab, clone the GIT repository and install it's dependencies:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%shell\r\n",
    "\r\n",
    "# Check if the repository is already available, if not, clone and install\r\n",
    "if [ ! -d .git ]\r\n",
    "then\r\n",
    "  git clone https://github.com/Rick-v-E/automatic_discard_registration.git\r\n",
    "  pip install -r automatic_discard_registration/requirements.txt\r\n",
    "  pip install -r automatic_discard_registration/detection/yolov3/requirements.txt\r\n",
    "  pip install automatic_discard_registration/detection/apex\r\n",
    "  pip install gdown\r\n",
    "else\r\n",
    "  git pull\r\n",
    "fi"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you installed the software in the previous step, enter the repository:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%cd automatic_discard_registration"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup dataset\r\n",
    "The complete dataset can be downloaded from [4TU.ResearchData](https://doi.org/10.4121/16622566.v1). To use this dataset, extract both `fdf_images.zip` and `results.zip` in the [data](https://github.com/Rick-v-E/automatic_discard_registration/tree/master/data) folder.\r\n",
    "\r\n",
    "For use on Google Colab, we have created a smaller subset of the data. This dataset contains only part of the images of the complete dataset, but contains all result from the complete dataset.\r\n",
    "\r\n",
    "---\r\n",
    "**IMPORTANT**\r\n",
    "\r\n",
    "Execute only one of the three cells below! Each cell contains a method to import the data, if one method fails, use another method. If the method succeed, go to the next section in this notebook.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "**METHOD 1** Download and extract the sample dataset (this will take around 5-10 minutes):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!gdown --id 1TcyeeX0UjhWldbjhLkCRJIuktDNeAMJJ\r\n",
    "!unzip -q fdf_sample_dataset.zip -d data\r\n",
    "!rm fdf_sample_dataset.zip"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**METHOD 2** Download the [sample dataset](https://drive.google.com/file/d/1TcyeeX0UjhWldbjhLkCRJIuktDNeAMJJ/view?usp=sharing) manually and upload it to Google Colab in the `automatic_discard_registration` opening the files tab and right click on the folder name:\r\n",
    "\r\n",
    "![Manual upload image](colab_manual_upload.png)\r\n",
    "\r\n",
    "After uploading, extract the dataset:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!unzip -q fdf_sample_dataset.zip -d data\r\n",
    "!rm fdf_sample_dataset.zip"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**METHOD 3** Download the [sample dataset](https://drive.google.com/file/d/1TcyeeX0UjhWldbjhLkCRJIuktDNeAMJJ/view?usp=sharing) and upload it to your personal Google Drive account. Connect this account to Google Colab:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from google.colab import drive\r\n",
    "drive.mount('/content/drive')\r\n",
    "\r\n",
    "!unzip -q ../drive/MyDrive/fdf_sample_dataset.zip -d data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check if the dataset is loaded correctly:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pathlib import Path\r\n",
    "\r\n",
    "DATA_PATH = Path(\"data\")\r\n",
    "NEEDED_FOLDERS = [\"fdf_images\", \"results\"]\r\n",
    "\r\n",
    "# Check if all folders are correct\r\n",
    "if not all([(DATA_PATH / f).is_dir() for f in NEEDED_FOLDERS]):\r\n",
    "    print(\"Could not find all data folders! Did you extract both fdf_images.zip and results.zip in the data folder?\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To get the same results as in the paper, use the complete dataset. Upload the dataset to your Google Drive and [mount](https://towardsdatascience.com/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166) this folder to your Google Colab environment. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup tracking notebook\r\n",
    "Start by importing the dependencies:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib notebook\r\n",
    "\r\n",
    "import cv2\r\n",
    "\r\n",
    "from pathlib import Path\r\n",
    "from IPython.display import HTML\r\n",
    "from tqdm.notebook import tqdm\r\n",
    "\r\n",
    "import matplotlib\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import matplotlib.animation as animation\r\n",
    "\r\n",
    "from common.nb_utils import show_image_with_trackers\r\n",
    "from common.io import load_detection_file, load_image_file_names, write_trackers_to_json\r\n",
    "from tracking.multi_tracker import MultiTracker\r\n",
    "from tracking.metrics.iou import IoUMetric"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setup all paths to the detection files and images folders. On the example dataset, not all detection files and image folders are available. If you are running on the complete dataset, you can uncomment these lines."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "path_dict = {\r\n",
    "    \"20191018_run_1_images\": Path(\"data/fdf_images/images/test_sequences/20191018_run_1\"),\r\n",
    "    # \"20191018_run_2_images\": Path(\"data/fdf_images/images/test_sequences/20191018_run_2\"),\r\n",
    "    # \"20191018_run_3_images\": Path(\"data/fdf_images/images/test_sequences/20191018_run_3\"),\r\n",
    "    # \"20191018_run_4_images\": Path(\"data/fdf_images/images/test_sequences/20191018_run_4\"),\r\n",
    "    # \"20191025_run_1_images\": Path(\"data/fdf_images/images/test_sequences/20191025_run_1\"),\r\n",
    "    # \"20191025_run_2_images\": Path(\"data/fdf_images/images/test_sequences/20191025_run_2\"),\r\n",
    "    # \"20191025_run_3_images\": Path(\"data/fdf_images/images/test_sequences/20191025_run_3\"),\r\n",
    "    # \"20191025_run_4_images\": Path(\"data/fdf_images/images/test_sequences/20191025_run_4\"),\r\n",
    "    # \"20191101_run_1_images\": Path(\"data/fdf_images/images/test_sequences/20191101_run_1\"),\r\n",
    "    # \"20191101_run_2_images\": Path(\"data/fdf_images/images/test_sequences/20191101_run_2\"),\r\n",
    "    # \"20191101_run_3_images\": Path(\"data/fdf_images/images/test_sequences/20191101_run_3\"),\r\n",
    "    # \"20191101_run_4_images\": Path(\"data/fdf_images/images/test_sequences/20191101_run_4\"),\r\n",
    "    \"20191018_run_1_detections\": Path(\"data/results/EM_comparison/20191018_run_1_detections.json\"),\r\n",
    "    # \"20191018_run_2_detections\": Path(\"data/results/EM_comparison/20191018_run_2_detections.json\"),\r\n",
    "    # \"20191018_run_3_detections\": Path(\"data/results/EM_comparison/20191018_run_3_detections.json\"),\r\n",
    "    # \"20191018_run_4_detections\": Path(\"data/results/EM_comparison/20191018_run_4_detections.json\"),\r\n",
    "    # \"20191025_run_1_detections\": Path(\"data/results/EM_comparison/20191025_run_1_detections.json\"),\r\n",
    "    # \"20191025_run_2_detections\": Path(\"data/results/EM_comparison/20191025_run_2_detections.json\"),\r\n",
    "    # \"20191025_run_3_detections\": Path(\"data/results/EM_comparison/20191025_run_3_detections.json\"),\r\n",
    "    # \"20191025_run_4_detections\": Path(\"data/results/EM_comparison/20191025_run_4_detections.json\"),\r\n",
    "    # \"20191101_run_1_detections\": Path(\"data/results/EM_comparison/20191101_run_1_detections.json\"),\r\n",
    "    # \"20191101_run_2_detections\": Path(\"data/results/EM_comparison/20191101_run_2_detections.json\"),\r\n",
    "    # \"20191101_run_3_detections\": Path(\"data/results/EM_comparison/20191101_run_3_detections.json\"),\r\n",
    "    # \"20191101_run_4_detections\": Path(\"data/results/EM_comparison/20191101_run_4_detections.json\"),\r\n",
    "}\r\n",
    "\r\n",
    "assert all([f.is_file() for n, f in path_dict.items() if \"detections\" in n])\r\n",
    "assert all([f.is_dir() for n, f in path_dict.items() if \"images\" in n])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the first sequence:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "images_dict = load_image_file_names(path_dict[\"20191018_run_1_images\"])\r\n",
    "detection_dict = load_detection_file(path_dict[\"20191018_run_1_detections\"])\r\n",
    "\r\n",
    "print(f\"Loaded { len(images_dict) } images with { len(detection_dict) } annotations\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the evaluation metric for the tracker:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metric = IoUMetric\r\n",
    "metric.threshold = 0.2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the tracker for the first sequence:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "first_image = list(images_dict.values())[0]  # Needed to initialize the ORB translation estimator\r\n",
    "tracker = MultiTracker(first_image, metric=metric, min_create_objectness_threshold=0.01, max_age=1, n_init=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loop over all detections and track the fishes over the images. In real-time, this will work faster since we don't have to load the image (which is time-consuming)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "min_objectness_threshold = 0.01\r\n",
    "\r\n",
    "for image_name, image_path in tqdm(images_dict.items(), desc=\"Tracking fishes\"):\r\n",
    "    image = cv2.imread(str(image_path))\r\n",
    "\r\n",
    "    if image is None:\r\n",
    "        print(f\"Could not read image { image_path }!\")\r\n",
    "        continue\r\n",
    "\r\n",
    "    detections = [dt for dt in detection_dict[image_name] if dt.is_certain(min_objectness_threshold)]\r\n",
    "    \r\n",
    "    # Predict the position of the detections in a next image\r\n",
    "    tracker.predict(image_name)\r\n",
    "\r\n",
    "    # Update the position\r\n",
    "    tracker.update(detections, image, image_name)\r\n",
    "\r\n",
    "trackers = tracker.get_trackers()\r\n",
    "\r\n",
    "print(f\"Tracked { len(trackers) } fishes over { len(images_dict) } images...\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save all the trackers as a `.json` file:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trackers_file = Path(\"data/20191018_run_1_tracking.json\")\r\n",
    "write_trackers_to_json(trackers_file, trackers)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make a visualisation of the images:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "matplotlib.rcParams[\"animation.embed_limit\"] = 2e5\r\n",
    "fig, ax = plt.subplots()\r\n",
    "ims = []\r\n",
    "\r\n",
    "trks = load_detection_file(trackers_file)\r\n",
    "for i, (name, path) in tqdm(enumerate(images_dict.items()), desc=\"Rendering annimation\"):\r\n",
    "    image = cv2.imread(str(path))\r\n",
    "    im = show_image_with_trackers(image, trks[name], ax)\r\n",
    "    ims.append([im])\r\n",
    "\r\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=200, blit=True, repeat_delay=1000)\r\n",
    "HTML(ani.to_html5_video())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Track all the sequences if you are using the complete dataset:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sequences = [\"20191018_run_2\", \"20191018_run_3\", \"20191018_run_4\", \"20191025_run_1\", \"20191025_run_2\", \"20191025_run_3\", \"20191025_run_4\", \"20191101_run_1\", \"20191101_run_2\", \"20191101_run_3\", \"20191101_run_4\"]\r\n",
    "for subset_name in tqdm(sequences, desc=\"Tracking subset\"):\r\n",
    "    images_dict = load_image_file_names(path_dict[f\"{ subset_name }_images\"])\r\n",
    "    detection_dict = load_detection_file(path_dict[f\"{ subset_name }_detections\"])\r\n",
    "\r\n",
    "    metric = IoUMetric\r\n",
    "    metric.threshold = 0.2\r\n",
    "\r\n",
    "    first_image = list(images_dict.values())[0]  # Needed to initialize the ORB translation estimator\r\n",
    "    tracker = MultiTracker(first_image, metric=metric, min_create_objectness_threshold=0.01, max_age=1, n_init=2)\r\n",
    "\r\n",
    "    min_objectness_threshold = 0.01\r\n",
    "\r\n",
    "    for image_name, image_path in tqdm(images_dict.items(), desc=\"Tracking fishes\"):\r\n",
    "        image = cv2.imread(str(image_path))\r\n",
    "\r\n",
    "        if image is None:\r\n",
    "            print(f\"Could not read image { image_path }!\")\r\n",
    "            continue\r\n",
    "\r\n",
    "        detections = [dt for dt in detection_dict[image_name] if dt.is_certain(min_objectness_threshold)]\r\n",
    "        \r\n",
    "        # Predict the position of the detections in a next image\r\n",
    "        tracker.predict(image_name)\r\n",
    "\r\n",
    "        # Update the position\r\n",
    "        tracker.update(detections, image, image_name)\r\n",
    "\r\n",
    "    trackers = tracker.get_trackers()\r\n",
    "\r\n",
    "    trackers_file = Path(f\"data/{ subset_name }_tracking.json\")\r\n",
    "    write_trackers_to_json(trackers_file, trackers) \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "All tracking files are now saved in the `data` folder. In the [evaluation notebook](https://github.com/Rick-v-E/automatic_discard_registration/blob/master/evaluate.ipynb), we use 'our' tracking files."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "9756859299261392d218088cb417af53d94c7bf0c66d0aa80032af89e52b4e36"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}