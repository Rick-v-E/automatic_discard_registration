{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the neural network\n",
    "In this paper, the [YOLOv3](https://arxiv.org/abs/1804.02767) neural network is used for fish detection. The implementation from [Ultralytics](https://github.com/ultralytics/yolov3) in Pytorch was for us the most conventient to use.\n",
    "\n",
    "## Setup GPU\n",
    "When using Google Colab, setup the hardware accelerator to use a GPU by:\n",
    "\n",
    "**Edit** > **Notebook settings** > **Hardware accelerator**.\n",
    "\n",
    "![Change Colab to use GPU](colab_gpu.png)\n",
    "\n",
    "## Install software\n",
    "To install the software on your own computer, follow the steps provided in the [readme](https://github.com/Rick-v-E/automatic_discard_registration/blob/master/README.md). If running on Google Colab, clone the GIT repository and install it's dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "# Check if the repository is already available, if not, clone and install\n",
    "if [ ! -d .git ]\n",
    "then\n",
    "  git clone --recurse-submodules https://github.com/WUR-ABE/automatic_discard_registration.git\n",
    "  pip install -r automatic_discard_registration/requirements.txt\n",
    "  pip install -r automatic_discard_registration/detection/yolov3/requirements.txt\n",
    "  pip install automatic_discard_registration/detection/apex\n",
    "  pip install gdown\n",
    "else\n",
    "  git pull\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you installed the software in the previous step, enter the repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd automatic_discard_registration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup dataset\n",
    "The complete dataset can be downloaded from [4TU.ResearchData](https://doi.org/10.4121/16622566.v1). To use this dataset, extract both `fdf_images.zip` and `results.zip` in the [data](https://github.com/Rick-v-E/automatic_discard_registration/tree/master/data) folder.\n",
    "\n",
    "For use on Google Colab, we have created a smaller subset of the data. This dataset contains only part of the images of the complete dataset, but contains all result from the complete dataset.\n",
    "\n",
    "---\n",
    "**IMPORTANT**\n",
    "\n",
    "Execute only one of the three cells below! Each cell contains a method to import the data, if one method fails, use another method. If the method succeed, go to the next section in this notebook.\n",
    "\n",
    "---\n",
    "\n",
    "**METHOD 1** Download and extract the sample dataset (this will take around 5-10 minutes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown --id 1TcyeeX0UjhWldbjhLkCRJIuktDNeAMJJ\n",
    "!unzip -q fdf_sample_dataset.zip -d data\n",
    "!rm fdf_sample_dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**METHOD 2** Download the [sample dataset](https://drive.google.com/file/d/1TcyeeX0UjhWldbjhLkCRJIuktDNeAMJJ/view?usp=sharing) manually and upload it to Google Colab in the `automatic_discard_registration` opening the files tab and right click on the folder name:\n",
    "\n",
    "![Manual upload image](colab_manual_upload.png)\n",
    "\n",
    "After uploading, extract the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q fdf_sample_dataset.zip -d data\n",
    "!rm fdf_sample_dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**METHOD 3** Download the [sample dataset](https://drive.google.com/file/d/1TcyeeX0UjhWldbjhLkCRJIuktDNeAMJJ/view?usp=sharing) and upload it to your personal Google Drive account. Connect this account to Google Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!unzip -q ../drive/MyDrive/fdf_sample_dataset.zip -d data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the dataset is loaded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "NEEDED_FOLDERS = [\"fdf_images\", \"results\"]\n",
    "\n",
    "# Check if all folders are correct\n",
    "if not all([(DATA_PATH / f).is_dir() for f in NEEDED_FOLDERS]):\n",
    "    print(\"Could not find all data folders! Did you extract both fdf_images.zip and results.zip in the data folder?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the same results as in the paper, use the complete dataset. Upload the dataset to your Google Drive and [mount](https://towardsdatascience.com/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166) this folder to your Google Colab environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training notebook\n",
    "Start by importing the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from detection import setup_paths\n",
    "from detection.train import train\n",
    "from detection.data_loader import FDFLoader\n",
    "\n",
    "from common.nb_utils import show_random_image\n",
    "from detection.yolov3.models import attempt_download\n",
    "from detection.yolov3.utils.utils import load_classes\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if we have a GPU available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print(\"No GPU device found! If you are working on Google Colab, make sure that you select the GPU hardware accelerator in the notebook settings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup all model parameters\n",
    "Now, define all the model parameters. There are hyperparameters for the model and configuration parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp = {\n",
    "    \"giou\": 3.54,  # giou loss gain\n",
    "    \"cls\": 37.4,  # cls loss gain\n",
    "    \"cls_pw\": 1.0,  # cls BCELoss positive_weight\n",
    "    \"obj\": 64.3,  # obj loss gain (*=img_size/320 if img_size != 320)\n",
    "    \"obj_pw\": 1.0,  # obj BCELoss positive_weight\n",
    "    \"iou_t\": 0.20,  # iou training threshold\n",
    "    \"lr0\": 0.01,  # initial learning rate (SGD=5E-3, Adam=5E-4)\n",
    "    \"lrf\": 0.0005,  # final learning rate (with cos scheduler)\n",
    "    \"momentum\": 0.937,  # SGD momentum\n",
    "    \"weight_decay\": 0.0005,  # optimizer weight decay\n",
    "    \"fl_gamma\": 0.0,  # focal loss gamma (efficientDet default is gamma=1.5)\n",
    "    \"hsv_h\": 0.0138,  # image HSV-Hue augmentation (fraction)\n",
    "    \"hsv_s\": 0.678,  # image HSV-Saturation augmentation (fraction)\n",
    "    \"hsv_v\": 0.36,  # image HSV-Value augmentation (fraction)\n",
    "    \"degrees\": 1.98 * 0,  # image rotation (+/- deg)\n",
    "    \"translate\": 0.05 * 0,  # image translation (+/- fraction)\n",
    "    \"scale\": 0.05 * 0,  # image scale (+/- gain)\n",
    "    \"shear\": 0.641 * 0,\n",
    "} \n",
    "\n",
    "opt = {\n",
    "    \"epochs\": 100,  # Number of epochs. We did 800 epochs on complete dataset\n",
    "    \"batch_size\": 8, \n",
    "    \"multi_scale\": False,  # adjust (67%% - 150%%) img_size every 10 batches\n",
    "    \"img_size\": [320, 640, 416],  # [min-train, max-train, test] image size\n",
    "    \"rect\": False,  # rectangular training\n",
    "    \"resume\": False,  # resume traning from last.pt\n",
    "    \"nosave\": False,  # only save final epoch\n",
    "    \"notest\": False,  # only test final epoch\n",
    "    \"evolve\": False,  # evolve hyperparameters\n",
    "    \"bucket\": \"\",  # gsutil bucket\n",
    "    \"cache_images\": False,  # cache images for faster training\n",
    "    \"name\": \"\",  # renames results.txt to results_name.txt if supplied\n",
    "    \"adam\": False,  # use adam optimizer\n",
    "    \"single_cls\": False,  # train as single-class dataset\n",
    "    \"freeze_layers\": False,  # freeze non-output layers,\n",
    "    \"name\": \"FDF_training\",    \n",
    "    \"device\": \"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "Now, load the dataset from the data folder. First, load the folders as variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path(\"data\")\n",
    "weights_folder = data_folder / \"results/model_weights\"\n",
    "dataset_folder = data_folder / \"fdf_images\"\n",
    "\n",
    "# Define files\n",
    "paths = {\n",
    "    \"weights_folder\": data_folder,\n",
    "    \"start_weights_file\": weights_folder / \"yolov3-spp-ultralytics.pt\",\n",
    "    \"best_weights_file\": data_folder / \"best.pt\",\n",
    "    \"last_weights_file\": data_folder / \"last.pt\",\n",
    "    \"results_file\": data_folder / \"training_results.txt\",\n",
    "    \"cfg_file\": Path(\"detection\") / \"yolov3-spp-fdf.cfg\",\n",
    "    \"names_file\": Path(\"detection\") / \"fish_classes.names\",\n",
    "}\n",
    "\n",
    "# Create folder if not exists\n",
    "weights_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check if files exists\n",
    "assert paths[\"cfg_file\"].is_file()\n",
    "assert paths[\"names_file\"].is_file()\n",
    "assert dataset_folder.is_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to download the pre-trained weights file from [Ultralytics](https://drive.google.com/drive/folders/1LezFG5g3BCW6iYaV89B2i64cqEUZD7e0). If this does not work, try to install `curl`:\n",
    "\n",
    "`sudo apt-get install curl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attempt_download(str(paths[\"start_weights_file\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the image classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = load_classes(paths[\"names_file\"])\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load the dataset images and annotations. Since we are using a custom annotation format (containing addional information like fish orientation, occlusion and an unique id), we use our custom dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check and recalculate image size\n",
    "imgsz_min, imgsz_max, imgsz_test = opt[\"img_size\"] \n",
    "\n",
    "# Image Sizes\n",
    "gs = 32  # (pixels) grid size\n",
    "assert math.fmod(imgsz_min, gs) == 0, \"--img-size %g must be a %g-multiple\" % (imgsz_min, gs)\n",
    "opt[\"multi_scale\"] |= imgsz_min != imgsz_max  # multi if different (min, max)\n",
    "if opt[\"multi_scale\"]:\n",
    "    if imgsz_min == imgsz_max:\n",
    "        imgsz_min //= 1.5\n",
    "        imgsz_max //= 0.667\n",
    "    grid_min, grid_max = imgsz_min // gs, imgsz_max // gs\n",
    "    imgsz_min, imgsz_max = int(grid_min * gs), int(grid_max * gs)\n",
    "img_size = imgsz_max \n",
    "\n",
    "train_dataset = FDFLoader(\n",
    "    dataset_folder,\n",
    "    classes,\n",
    "    subset_name=\"train\",\n",
    "    img_size=img_size,\n",
    "    batch_size=opt[\"batch_size\"],\n",
    "    augment=True,\n",
    "    hyp=hyp,\n",
    "    rect=opt[\"rect\"],\n",
    "    cache_images=opt[\"cache_images\"],\n",
    "    single_cls=opt[\"single_cls\"],\n",
    ")\n",
    "test_dataset = FDFLoader(\n",
    "    dataset_folder,\n",
    "    classes,\n",
    "    subset_name=\"validation\",\n",
    "    img_size=imgsz_test,\n",
    "    batch_size=min(opt[\"batch_size\"], len(train_dataset)),\n",
    "    hyp=hyp,\n",
    "    rect=True,\n",
    "    cache_images=opt[\"cache_images\"],\n",
    "    single_cls=opt[\"single_cls\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, show an random training and testing image with annotations to make sure that we have loaded the dataset correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1, 2, figsize=(20,20))\n",
    "show_random_image(train_dataset, axarr[0], classes)\n",
    "show_random_image(test_dataset, axarr[1], classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "First step is to launch Tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start Tensorboard with \"tensorboard --logdir=data/runs --bind_all\", view at http://localhost:6006/')\n",
    "tb = torch.utils.tensorboard.SummaryWriter(comment=opt[\"name\"], log_dir=\"data/runs/{}\".format(opt[\"name\"]))\n",
    "%tensorboard --logdir data/runs/FDF_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_dataset, test_dataset, paths, hyp, opt, tb_writer=tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best epoch was selected using the accuracy. The corresponding weights are saved in `data/best.pt`. For the next notebooks, we are using the weights trained on the complete dataset."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9756859299261392d218088cb417af53d94c7bf0c66d0aa80032af89e52b4e36"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
