{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Detect the fishes in the images\r\n",
    "This notebook detects the fishes in images. This requires a trained YOLOv3 network.\r\n",
    "\r\n",
    "## Setup GPU\r\n",
    "When using Google Colab, setup the hardware accelerator to use a GPU by:\r\n",
    "\r\n",
    "**Edit** > **Notebook settings** > **Hardware accelerator**.\r\n",
    "\r\n",
    "![Change Colab to use GPU](colab_gpu.png)\r\n",
    "\r\n",
    "## Install software\r\n",
    "To install the software on your own computer, follow the steps provided in the [readme](https://github.com/Rick-v-E/automatic_discard_registration/blob/master/README.md). If running on Google Colab, clone the GIT repository and install it's dependencies:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%shell\r\n",
    "\r\n",
    "# Check if the repository is already available, if not, clone and install\r\n",
    "if [ ! -d .git ]\r\n",
    "then\r\n",
    "  git clone https://github.com/Rick-v-E/automatic_discard_registration.git\r\n",
    "  pip install -r automatic_discard_registration/requirements.txt\r\n",
    "  pip install -r automatic_discard_registration/detection/yolov3/requirements.txt\r\n",
    "  pip install automatic_discard_registration/detection/apex\r\n",
    "  pip install gdown\r\n",
    "else\r\n",
    "  git pull\r\n",
    "fi"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you installed the software in the previous step, enter the repository:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%cd automatic_discard_registration"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup dataset\r\n",
    "The complete dataset can be downloaded from [4TU.ResearchData](https://doi.org/10.4121/16622566.v1). To use this dataset, extract both `fdf_images.zip` and `results.zip` in the [data](https://github.com/Rick-v-E/automatic_discard_registration/tree/master/data) folder.\r\n",
    "\r\n",
    "For use on Google Colab, we have created a smaller subset of the data. This dataset contains only part of the images of the complete dataset, but contains all result from the complete dataset.\r\n",
    "\r\n",
    "---\r\n",
    "**IMPORTANT**\r\n",
    "\r\n",
    "Execute only one of the three cells below! Each cell contains a method to import the data, if one method fails, use another method. If the method succeed, go to the next section in this notebook.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "**METHOD 1** Download and extract the sample dataset (this will take around 5-10 minutes):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!gdown --id 1TcyeeX0UjhWldbjhLkCRJIuktDNeAMJJ\r\n",
    "!unzip -q fdf_sample_dataset.zip -d data\r\n",
    "!rm fdf_sample_dataset.zip"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**METHOD 2** Download the [sample dataset](https://drive.google.com/file/d/1TcyeeX0UjhWldbjhLkCRJIuktDNeAMJJ/view?usp=sharing) manually and upload it to Google Colab in the `automatic_discard_registration` opening the files tab and right click on the folder name:\r\n",
    "\r\n",
    "![Manual upload image](colab_manual_upload.png)\r\n",
    "\r\n",
    "After uploading, extract the dataset:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!unzip -q fdf_sample_dataset.zip -d data\r\n",
    "!rm fdf_sample_dataset.zip"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**METHOD 3** Download the [sample dataset](https://drive.google.com/file/d/1TcyeeX0UjhWldbjhLkCRJIuktDNeAMJJ/view?usp=sharing) and upload it to your personal Google Drive account. Connect this account to Google Colab:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from google.colab import drive\r\n",
    "drive.mount('/content/drive')\r\n",
    "\r\n",
    "!unzip -q ../drive/MyDrive/fdf_sample_dataset.zip -d data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check if the dataset is loaded correctly:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pathlib import Path\r\n",
    "\r\n",
    "DATA_PATH = Path(\"data\")\r\n",
    "NEEDED_FOLDERS = [\"fdf_images\", \"results\"]\r\n",
    "\r\n",
    "# Check if all folders are correct\r\n",
    "if not all([(DATA_PATH / f).is_dir() for f in NEEDED_FOLDERS]):\r\n",
    "    print(\"Could not find all data folders! Did you extract both fdf_images.zip and results.zip in the data folder?\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To get the same results as in the paper, use the complete dataset. Upload the dataset to your Google Drive and [mount](https://towardsdatascience.com/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166) this folder to your Google Colab environment. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup detection notebook\r\n",
    "Start by loading the dependencies:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\r\n",
    "\r\n",
    "import cv2\r\n",
    "import torch\r\n",
    "import warnings\r\n",
    "\r\n",
    "from pathlib import Path\r\n",
    "from tqdm.notebook import tqdm\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "\r\n",
    "from detection import setup_paths\r\n",
    "from detection.detect import FDFDetector\r\n",
    "from common.io import load_image_file_names, write_detections_to_json\r\n",
    "from common.nb_utils import show_random_image_with_detection\r\n",
    "\r\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check if we have a GPU available:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if not torch.cuda.is_available():\r\n",
    "    print(\"No GPU device found! If you are working on Google Colab, make sure that you select the GPU hardware accelerator in the notebook settings.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, load all images in the validation and / or the test image folder:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "validation_images_path = Path(\"data/fdf_images/images/validation\")\r\n",
    "test_images_path = Path(\"data/fdf_images/images/test\")\r\n",
    "\r\n",
    "assert validation_images_path.is_dir()\r\n",
    "assert test_images_path.is_dir()\r\n",
    "\r\n",
    "validation_images = load_image_file_names(validation_images_path)\r\n",
    "test_images = load_image_file_names(test_images_path)\r\n",
    "\r\n",
    "print(f\"Loaded { len(validation_images) } validation and { len(test_images) } test images...\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are four trained weights files available in the dataset:\r\n",
    "\r\n",
    "| Filename                     | Number of synthetic images |\r\n",
    "|------------------------------|----------------------------|\r\n",
    "| weights_0_synthetic_images   | 0                          |\r\n",
    "| weights_50_synthetic_images  | 50                         |\r\n",
    "| weights_100_synthetic_images | 100                        |\r\n",
    "| weights_200_synthetic_images | 200                        |\r\n",
    "\r\n",
    "For all 4 files, we should detect the fishes in the validation dataset. Based on the performance in the validation dataset, we choose the best working model and detect the fishes in the test dataset. \r\n",
    "\r\n",
    "Now, create a dictionary with all the files needed for the detection:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "path_dict = {\r\n",
    "    \"weights_file_0\": Path(\"data/results/model_weights/weights_0_synthetic_images.pt\"),\r\n",
    "    \"weights_file_50\": Path(\"data/results/model_weights/weights_50_synthetic_images.pt\"),\r\n",
    "    \"weights_file_100\": Path(\"data/results/model_weights/weights_100_synthetic_images.pt\"),\r\n",
    "    \"weights_file_200\": Path(\"data/results/model_weights/weights_200_synthetic_images.pt\"),\r\n",
    "    \"cfg_file\": Path(\"detection/yolov3-spp-fdf.cfg\"),\r\n",
    "    \"names_file\": Path(\"detection/fish_classes.names\")\r\n",
    "}\r\n",
    "\r\n",
    "assert all(f.is_file() for f in path_dict.values())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Detect fishs in image\n",
    "Loop over all validation images, detect the fishes and save the results to a `.json` file. Do this for the models with 0, 50, 100 and 200 synthetic images."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for n_synthetic_images in tqdm([0, 50, 100, 200], desc=\"Models\"):\r\n",
    "    detector = FDFDetector(path_dict[f\"weights_file_{ n_synthetic_images }\"], path_dict)\r\n",
    "    detection_dict = {}\r\n",
    "    for image_name, image_path in tqdm(validation_images.items(), desc=\"Detecting fishes\", leave=False):\r\n",
    "        image = cv2.imread(str(image_path))\r\n",
    "        detection_dict[image_name] = detector.detect(image, image_path)\r\n",
    "\r\n",
    "    output_file = Path(f\"data/validation_detections_{ n_synthetic_images }_synthetic_images.json\")\r\n",
    "    write_detections_to_json(output_file, detection_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check two output images of the last model with 200 synthetic images:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f, axarr = plt.subplots(1, 2, figsize=(20,20))\r\n",
    "show_random_image_with_detection(validation_images, detection_dict, axarr[0])\r\n",
    "show_random_image_with_detection(validation_images, detection_dict, axarr[1])\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We know that 200 synthetic images works best (see [evaluation script](https://github.com/Rick-v-E/automatic_discard_registration/blob/master/evaluate.ipynb)), so we use that model. If you want, you can select one of the other models. Now, detect the fish in the images:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "detector = FDFDetector(path_dict[f\"weights_file_200\"], path_dict)\r\n",
    "detection_dict = {}\r\n",
    "for image_name, image_path in tqdm(test_images.items(), desc=\"Detecting fishes\", leave=False):\r\n",
    "    image = cv2.imread(str(image_path))\r\n",
    "    detection_dict[image_name] = detector.detect(image, image_path)\r\n",
    "\r\n",
    "output_file = Path(\"data/test_detections_200_synthethic_images.json\")\r\n",
    "write_detections_to_json(output_file, detection_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot two images with detected fishes:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f, axarr = plt.subplots(1, 2, figsize=(20,20))\r\n",
    "show_random_image_with_detection(test_images, detection_dict, axarr[0])\r\n",
    "show_random_image_with_detection(test_images, detection_dict, axarr[1])\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The detection results (position of bounding box, objectness and class confidences) of the test dataset are saved in `data/test_detections_200_synthethic_images.json`. For the other notebooks, we use 'our' results file."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "8c0bc8a0ae024fce12f7ef6edea13e519ee666eb85275fe7a37d7f4c984c5c3b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}